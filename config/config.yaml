data:
  data_dir: "data"
  # "all" will load all .npz files in data_dir
  shots: "all"

  # Uniform grid only
  rho_grid_mode: "uniform"
  # Optional. If omitted, uses the NPZ rho length.
  # uniform_n_rho: 26

  # BC
  edge_bc_mode: "use_last_observed"

system:
  device: "gpu"

training:
  # Longer, higher-quality run
  num_restarts: 1

  optimizer: "adamw"
  learning_rate: 2.0e-4
  weight_decay: 3.0e-5

  # Longer AdamW phase before L-BFGS
  warmup_steps: 5
  total_steps: 100
  batch_size: 8
  grad_clip: 1.0e3

  # logging
  log_every: 10

  # loss weights (explicit)
  huber_delta: 10.0
  lambda_src: 1.0e-4
  src_delta: 10.0
  lambda_w: 0.0
  model_error_delta: 20.0
  lambda_z: 1.0e-4
  lambda_zreg: 1.0e-5

  # ODE solver configuration
  solver: "imex"   # alternatives: kvaerno5, tsit5, kencarp3, imex
  rtol: 1.0e-3
  atol: 1.0e-3

  # IMEX-specific configuration (only used if solver=="imex")
  imex:
    theta: 0.7         # Less dissipative than 1.0, still stable for stiff diffusion
    dt_base: 0.001     # Base timestep (seconds)
    substeps: 5        # More substeps -> healthier sensitivities than one big implicit step
    max_steps: 50000   # Maximum number of steps
    rtol: 1.0e-4       # Relative tolerance (for future adaptive stepping)
    atol: 1.0e-6       # Absolute tolerance

  # Optional EMA (0 disables)
  ema_decay: 0.999

  # Enable a short, safe L-BFGS finetune after AdamW
  lbfgs_finetune: True  
  lbfgs_maxiter: 5
  lbfgs_history: 1
  lbfgs_batch_shots: 8
  lbfgs_tol: 5.0e-5

output:
  model_id: "production_run_v1"
  save_dir: "models"
  log_dir: "logs"
  model_name: "tokamak ode model"

model:
  layers: 128
  depth: 4
  latent_gain: 1.0
  source_scale: 3.0e5
